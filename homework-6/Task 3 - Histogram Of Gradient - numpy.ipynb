{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6: Appearance-based gaze estimation - Task 03\n",
    "\n",
    "Author: `Yintao, Xu` | Date: `2020-02-15` | Email: xuyt@shanghaitech.edu.cn\n",
    "\n",
    "Topics: `numpy`\n",
    "\n",
    "**Note**: Make sure you have completed task 01 & 02.\n",
    "\n",
    "HOG, as a feature engineering method, could improve your estimation performance.\n",
    "\n",
    "![](figures/HOG_visual.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: run this cell before runnng any cell to activate auto re-import\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 'gazelib' is the toolkit provided by this assignment, at the same directory of this notebook\n",
    "from gazelib.utils import *\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets has been supposed to be downloaded at last notebook\n",
    "# You could run thie cell to have a check :)\n",
    "download_csv_mpIIdataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training dataset\n",
    "train_df = load_train_csv_as_df()\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data into numpy arrays\n",
    "def df2nptensor(df):\n",
    "    imgs = []\n",
    "    imgs_HOG = []\n",
    "    gaze_dirs = []\n",
    "\n",
    "    print_interval = 1000\n",
    "    print_cnter = 0\n",
    "    \n",
    "    for _, i in df.iterrows():\n",
    "        if print_cnter % print_interval == 0:\n",
    "            print(\"[{} / {}]\".format(print_cnter, len(df)), end='\\r')\n",
    "        print_cnter += 1\n",
    "        im_arr = decode_base64_img(i['image_base64'])\n",
    "        gaze_dirs.append([i['yaw'], i['pitch']])\n",
    "        im = im_arr / 255\n",
    "        \n",
    "        imgs.append(im)\n",
    "    \n",
    "    gaze_dirs = np.array(gaze_dirs)\n",
    "    imgs = np.array(imgs)\n",
    "    \n",
    "    return gaze_dirs, imgs\n",
    "\n",
    "# For effciency, we only takes first 5,000 samples. Pick subject 5 as validation \n",
    "# set and the rest of the dataset as training set\n",
    "SAMPLE_NUM = 5000\n",
    "print(\"Start to generate sampled dataset, it may take ~10s.\")\n",
    "train_Y, train_X = df2nptensor(train_df[train_df[\"subject_id\"] != 5][: int(SAMPLE_NUM * 0.8)])\n",
    "val_Y, val_X = df2nptensor(train_df[train_df[\"subject_id\"] == 5][: int(SAMPLE_NUM * 0.2)])\n",
    "\n",
    "print(\"train_X.shape: {}\".format(train_X.shape))\n",
    "print(\"train_Y.shape: {}\".format(train_Y.shape))\n",
    "print(\"val_X.shape: {}\".format(val_X.shape))\n",
    "print(\"val_X.shape: {}\".format(val_Y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roadmap\n",
    "\n",
    "At previous sections, estimation pipeline for gaze vector has been built:\n",
    "\n",
    "<img src=\"figures/gaze_model_pipeline.png\" style=\"zoom:80%\" />\n",
    "\n",
    "Formalize the input/ouput of our estimation pipeline mathematically:\n",
    "- `Input`: the gray-scale image $I\\in \\mathbb{R}^{18\\times30}$ (18x30 numpy array)\n",
    "- `Output`: two floats: yaw($\\gamma \\in \\mathbb{R}$), pitch($\\theta  \\in \\mathbb{R}$)\n",
    "\n",
    "\n",
    "In previous sections, we have evaluated the simlarity among images by Euclidean distance. However, naive computation over raw image is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.0: Motivation: Why HOG? \n",
    "\n",
    "\n",
    "We have 42,000 images in our pandas data frame. Assume in each estimation, we require doing one float number multplication for each pixels in every image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.randn(42000 * 18 * 30,)\n",
    "b = np.random.randn(42000 * 18 * 30,)\n",
    "\n",
    "time_start = time.time()\n",
    "c = a * b\n",
    "time_end = time.time()\n",
    "\n",
    "print(\"Multiply 42000 * 18 * 30 numbers cost: {:.4f}s\".format(time_end - time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Large computational cost**: It runs ~0.06 seconds on *Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz*, which sounds tractable. However, you may repeat 10,000 ~ 20,000 such operation, which means roughly that it will cost ~ 20 min. Besides, memory cost is also considerable. Therefore, we require a **dimension reduction** method to reduce the computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Not robust to translation**: If we apply Euclidean distance to images, when a slight translation of image occurs, it may lead to dramatic change to output.\n",
    "\n",
    "In computer vision community, **HOG(histogram of oriented gradient)** is a frequently used method to do the feature engineering of images with tractable size of dimension and much robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Gradient of Image [2]\n",
    "\n",
    "There is a widely accepted assumption in computer vision community: \n",
    "> local object appearance and shape can often be characterized rather well by **the distribution of local intensity gradients or edge directions**, even without precise knowledge of the corresponding gradient or edge posistions.\n",
    "\n",
    "Therefore, studying the **image gradient**[3] is important. It is a directional change in the intensity or color in an image. \n",
    "\n",
    "![](figures/HOG.png)\n",
    "\n",
    "Mathematically, the gradient of a two-variables function (here the image intensity function) at each image point is a 2D vector with the components given by the derivatives in the horizontal and vertical directions.\n",
    "\n",
    "An image in modern devices can be viewed as a matrix of pixel intensity, which is discrete. However, derivatives of this function cannot be defined unless we assume that there is an underlying continuous intensity function which has been sampled at the image points. \n",
    "\n",
    "The most common way to approximate the image gradient is to **convolve an image with a kernel**, such as the [**Sobel operator**](https://en.wikipedia.org/wiki/Sobel_operator).\n",
    "\n",
    "Let's first start with **2D convolution**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1.1: 2D Convolution[1] (15%)\n",
    "Convolution is an important class of operators in linear time-invariant (LTI) system. You start with a **kernel**, which is simply a small matrix of weights. This kernel “slides” over the 2D input data, performing an element-wise multiplication with the part of the input it is currently on, and then summing up the results into a single output pixel.\n",
    "\n",
    "![ChessUrl](http://data.liubai01.cn:81/f/c518901817914d7f815e/?dl=1 \"chess\") \n",
    "\n",
    "Formally, it is denoted as $*$:\n",
    "\n",
    "$$\n",
    "    \\textbf{O} = \\textbf{F} * \\textbf{I}\n",
    "$$\n",
    "\n",
    "with $\\textbf{O}$ corresponding to output, $\\textbf{F}\\in \\mathbb{R}^{2K+1\\times2K+1}$ corresponding to filter kernel, $\\textbf{I}\\in \\mathbb{R}^{W \\times H}$ corresponding to the input Image.\n",
    "\n",
    "The formula can be written as follows:\n",
    "\n",
    "$$\n",
    "    \\textbf{O}_{i,j} = \\sum_{k=0}^{2K} \\sum_{l=0}^{2K} \\textbf{F}_{k,l} \\times \\textbf{I}_{k + i, l + j}, i \\in [0, W -2], j \\in [0, H - 2]\n",
    "$$\n",
    "\n",
    "For instance, for the upper-left corner of output value 12 at previous figure:\n",
    "\n",
    "$$\n",
    "    12 = 3 \\times 0 + 3 \\times 1 + 2 \\times 2 + 0 \\times 2 + 0 \\times 2 + 1 \\times 0 + 3 \\times 0 + 1 \\times 1 + 2 \\times 2\n",
    "$$\n",
    "\n",
    "*Remark: For those who has background in signal & system, note the inconsistent meaning of \"Conovolution\" in mathematical(signal&system) context and machine learning context. In most cases, convolution in machine learning is the correlation in signal & system, which means you are not required to flip the kernel. It was a historical misnormer here, but is accepted by most scientists nowadays.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete code at `AppearanceGazeEst/conv2d3x3`(15%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Section 3.1.1 (15%)\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert_eq_np\n",
    "from AppearanceGazeEst import conv2d3x3\n",
    "from gazelib.task_2_judge import assert_eq_np\n",
    "\n",
    "img = np.array([\n",
    "    [3, 3, 2, 1, 0],\n",
    "    [0, 0, 1, 3, 1],\n",
    "    [3, 1, 2, 2, 3],\n",
    "    [2, 0, 0, 2, 2],\n",
    "    [2, 0, 0, 0, 1]\n",
    "])\n",
    "kernel = np.array([\n",
    "    [0, 1, 2],\n",
    "    [2, 2, 0],\n",
    "    [0, 1, 2]\n",
    "])\n",
    "gt_out = np.array([\n",
    "    [12, 12, 17],\n",
    "    [10, 17, 19],\n",
    "    [9, 6, 14]\n",
    "])\n",
    "\n",
    "out = conv2d3x3(img, kernel)\n",
    "assert_eq_np(gt_out, out)\n",
    "\n",
    "print(\"Pass Section 3.1.1 - convolution 2d local test(15%).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.1.2: Compute Gradient By Sobel Operator (15%) - Apply convolution by numpy [4]\n",
    "The Sobel operator is a discrete differentiation operator, computing an approximation of the gradient of the image intensity function.\n",
    "\n",
    "The operator uses two 3×3 kernels which are convolved with the original image to calculate approximations of the derivatives – one for horizontal changes, and one for vertical. The 2D convolution has been implemented by previous sections. If we define I as the source image, and Gx and Gy are two images which at each point contain the vertical and horizontal derivative approximations respectively, the computations are as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf {G} _{x}={\\begin{bmatrix}-1&0&1\\\\-2&0&2\\\\-1&0&1\\end{bmatrix}}*\\mathbf {I} \\quad {\\mbox{and}}\\quad \\mathbf {G} _{y}={\\begin{bmatrix}-1&-2&-1\\\\0&0&0\\\\1&2&1\\end{bmatrix}}*\\mathbf {I}\n",
    "$$\n",
    "\n",
    "In most implementations, they also apply gaussian kernel to smooth the image before getting gradients.\n",
    "\n",
    "![](figures/eye_grad_new.png)\n",
    "\n",
    "Formally, for each pixel\n",
    "$$\n",
    "Mag = \\sqrt{g _{x}^2 + g_{y}^2}\n",
    "$$\n",
    "$$\n",
    "\\mathbf{Dir} = [g _{x}; g_{y}]\n",
    "$$\n",
    "$$\n",
    "\\mathbf{Dir}_{norm} = \\mathbf{Dir} / (Mag + 10^{-3})\n",
    "$$\n",
    "Here, 10e-3 is applied to improve numerical robustness in case of zero magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete the code of the sobel gradient computation at `AppearanceGazeEst.py/compute_grad`(15%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Section 3.1.2 (15%)\n",
    "# Note: feel free to print out your result to debug at gazelib.task_2_judge.py/assert_eq_2223\n",
    "# if it cannot pass assert_eq_np\n",
    "from AppearanceGazeEst import compute_grad\n",
    "from gazelib.task_2_judge import assert_eq_2223\n",
    "\n",
    "sanity_im = np.array([\n",
    "    [1., 1., 7.], \n",
    "    [2., 1., 8.], \n",
    "    [3., 5., 2.]\n",
    "])\n",
    "\n",
    "grad_dir, grad_mag = compute_grad(sanity_im)\n",
    "assert_eq_2223(grad_dir, grad_mag)\n",
    "print(\"You pass the local test - Section 3.1.2 (15%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample an image to visualize the gradient computation\n",
    "sample_im = train_X[random.randint(0, train_X.shape[0])]\n",
    "gaze_dir, gaze_mag = compute_grad(sample_im)\n",
    "vis.vis_grad(sample_im, gaze_dir[0] * gaze_mag, gaze_dir[1] * gaze_mag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 Histogram of oriented gradient (10%, Bonus 10%)\n",
    "Recall in section 2.3:\n",
    "> local object appearance and shape can often be characterized rather well by **the distribution of local intensity gradients or edge directions**, even without precise knowledge of the corresponding gradient or edge posistions.\n",
    "\n",
    "When finishing computing of gradients, at next step, you should get the distribution over an image (or an image patch). Using histogram is an intuitive way to model a distribution with many weighted data points(gradients of pixels, with direction and magnitude). This step is called **Orientation Binning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.1: Details of Orientation Binning (Optional Reading)\n",
    "\n",
    "**Note**: You are not required to go to very details of Orientation Binning. We'll provide you with a reference code.\n",
    "\n",
    "<img src=\"figures/hist_grad_vis.png_fix.png\" style=\"zoom:60%\" />\n",
    "\n",
    "**Range of angle**[6] In this assignment, angles are derived from `np.arctan2` towards gradient direction(x1,x2), which means that it ranges from $[-\\pi, +\\pi]$.\n",
    "\n",
    "![](figures/arctan2_wiki.png)\n",
    "\n",
    "**Bilinear voting policy**[5] A bin is selected based on the direction, and the vote ( the value that goes into the bin ) is selected based on the magnitude. \n",
    "\n",
    "Let’s first focus on the pixel encircled in blue. It has an angle ( direction ) of 30 degrees and magnitude of 5. So it adds 5 to the 30-deg. bin. The gradient at the pixel encircled using red has an angle of -40 degrees and magnitude of 3. Since -40 degrees is between -60 and -30, the vote by the pixel splits into the two bins \n",
    "proportional to 1/dist. to the bin.\n",
    "\n",
    "**Boundary condtion** When angle goes to boundary, for example, 160 degrees, it should be going to bin of 150-deg and -180-deg. Due to the range of `np.arctan2`, angle skips numerically at 180 and -180 but is adajacent at Cartesian coordination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.2: Numpy: vectorization [7]\n",
    "\n",
    "Vectorization is used to speed up the Python code without using loop. Numpy will invokes C-code directly at backend, which is more rapid than python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try this cell to see the power of vectorization\n",
    "a = np.random.randn(100000,)\n",
    "b = np.random.randn(100000,)\n",
    "\n",
    "c = 0\n",
    "# python loops\n",
    "time_start = time.time()\n",
    "for i in range(100000):\n",
    "    c = a[i] + b[i]\n",
    "time_nonvec = time.time() - time_start\n",
    "\n",
    "# vectorization\n",
    "time_start = time.time()\n",
    "c_ = np.sum(a + b)\n",
    "time_vec = time.time() - time_start\n",
    "\n",
    "speed_up_per = (time_nonvec / time_vec - 1) * 100\n",
    "\n",
    "print(\"Before vectorization: {:.3f} s\".format(time_nonvec))\n",
    "print(\"After vectorization: {:.3f} s\".format(time_vec))\n",
    "print(\"Speedup: {:.2f}%\".format(speed_up_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.3: Re-implement HOG - learn vectorization at numpy(Bonus, 10%)\n",
    "\n",
    "**Note**: the non-vectorized version is at `ApperanceGazeEstV2/bilinear_HOG_nonvec` above the target function `bilinear_HOG`. Please gaurantee that your implementation shares the same output as it and speeds up at least 200%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete the code of the HOG computation at `AppearanceGazeEst.py/bilinear_HOG`(10%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Section 3.2.3 (Bonus, 10%)\n",
    "# Note: feel free to print out your result to debug at gazelib.task_2_judge.py/assert_eq_2223\n",
    "# if it cannot pass assert_eq_np\n",
    "from gazelib.HOG import bilinear_HOG_patch_nonvec\n",
    "from AppearanceGazeEst import bilinear_HOG\n",
    "import time\n",
    "\n",
    "sample_im = train_X[5]\n",
    "grad_dir, grad_mag = compute_grad(sample_im)\n",
    "time_nonvec = 0\n",
    "time_vec = 0\n",
    "exp_num = 6\n",
    "\n",
    "for _ in range(exp_num): # repeat experiments due to randomness of OS scheduling\n",
    "    time_start = time.time()\n",
    "    a = bilinear_HOG_patch_nonvec(gaze_dir, gaze_mag)\n",
    "    time_nonvec += time.time() - time_start\n",
    "\n",
    "    time_start = time.time()\n",
    "    b = bilinear_HOG(gaze_dir, gaze_mag)\n",
    "    time_vec += time.time() - time_start\n",
    "\n",
    "speed_up_per = (time_nonvec / time_vec - 1) * 100\n",
    "assert_eq_np(a, b)\n",
    "assert speed_up_per > 200\n",
    "\n",
    "print(\"Before vectorization(avg): {:.3f} s\".format(time_nonvec / exp_num))\n",
    "print(\"After vectorization(avg): {:.3f} s\".format(time_vec / exp_num))\n",
    "print(\"Speedup: {:.2f}%\".format(speed_up_per))\n",
    "print(\"You pass the local test - Section 3.2.3 (Bonus, 10%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a random image\n",
    "random_idx = random.randint(0, train_X.shape[0])\n",
    "sample_im = train_X[random_idx]\n",
    "grad_dir, grad_mag = compute_grad(sample_im)\n",
    "\n",
    "hist = bilinear_HOG(grad_dir, grad_mag)\n",
    "\n",
    "vis.vis_HOG(sample_im, grad_dir[0], grad_dir[1], hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3.2.4: Descriptor blocks\n",
    "To keep spatial information, in practice, HOG splits image into blocks to get histograms, then concatenates histograms in a fixed order(e.g: row major) to a vector.\n",
    "\n",
    "<img src=\"figures/desc_block.png\" style=\"zoom:60%\" />\n",
    "\n",
    "**Note**: In this section, we give the implementation of descriptor blocks based on your previous function of HOG. If you do not complete the bonus(section 3.2.3), modify `bilinear_HOG_DB` function maually to use non-vectorization version of HOG. It uses the vectorization version by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AppearanceGazeEst import bilinear_HOG_DB\n",
    "\n",
    "random_idx = random.randint(0, train_X.shape[0])\n",
    "sample_im = train_X[random_idx]\n",
    "grad_dir, grad_mag = compute_grad(sample_im)\n",
    "\n",
    "# You can modify hyperparameter patch_num here, though not recommended\n",
    "patch_num = (3, 4)\n",
    "hist = bilinear_HOG_DB(sample_im, patch_num=patch_num)\n",
    "\n",
    "vis.vis_HOG_full(sample_im, grad_dir[0], grad_dir[1], hist, patch_num=patch_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3 Combine HOG with K-NN (5%)\n",
    "\n",
    "Recall at previous sections, we applied HOG to raw image to transform it into a vector. Let's apply HOG(completed on previous notebook) to all images here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we could do KNN over the HOGized images. You would see the improvement of performance with respect to the previous sections. Since our python-based implementation of HOG is inefficient, the estimation process may be a little bit long. \n",
    "![](figures/code_time.png)\n",
    "**Complete the code of the HOG computation at `ApperanceGazeEstV2/KNN_HOG`(5%)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Eye images\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert_eq_np\n",
    "from AppearanceGazeEst import KNN_HOG\n",
    "from gazelib.task_2_judge import assert_eq_np\n",
    "from gazelib.task_3_judge import compute_angle_error\n",
    "\n",
    "idx = 10\n",
    "# truncate the traininig set to improve the performance\n",
    "ret = KNN_HOG(train_X, train_Y, val_X[idx], 5)\n",
    "print(ret)\n",
    "assert_eq_np(ret, np.array([0.10192861, -0.06430973]))\n",
    "\n",
    "print(\"Pass local test@3.3 - eye images\")\n",
    "plt.figure(figsize=(4, 3))\n",
    "vis.visualize_est(val_X[idx], ret, val_Y[idx])\n",
    "plt.title(\"Angle Error: {:.3f}\".format(compute_angle_error(val_Y[idx], ret)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/good_job_banner.png)\n",
    "You should have completed **all cells(35% + 10% Bonus)** in this task locally when you reach here!\n",
    "\n",
    "**CheckList**\n",
    "\n",
    "- conv2d (15%)\n",
    "- compute_grad (15%)\n",
    "- bilinear_HOG (Bonus, 10%)\n",
    "- KNN_HOG (5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [1] Intuitively Understanding Convolutions for Deep Learning: https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\n",
    "- [2] Image gradient wiki: https://en.wikipedia.org/wiki/Image_gradient\n",
    "- [3] One of image gradient visualization is imported from: https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "- [4] https://en.wikipedia.org/wiki/Sobel_operator\n",
    "- [5] https://www.learnopencv.com/histogram-of-oriented-gradients/\n",
    "- [6] https://en.wikipedia.org/wiki/Atan2\n",
    "- [7] https://www.geeksforgeeks.org/vectorization-in-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
