{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![avatar](figures/front_banner_T1S0_new.png)\n",
    "\n",
    "# Assignment 6: Appearance-based gaze estimation - Task 01\n",
    "\n",
    "Author: `Yintao, Xu` | Date: `2020-02-12` | Email: xuyt@shanghaitech.edu.cn\n",
    "\n",
    "Topics: `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.1.1: Introduction\n",
    "\n",
    "> Gaze tracking or gaze estimation is an important topic for understanding human visual attention. Such a technology has been widely deployed in various fields, such as humanâ€“computer interaction, visual behavior analysis, and psychological studies. Based on its huge potential value, many eye tracking devices (e.g., Tobii X3-120, Tobii EyeX, and Eye Tribe) have come into being. However, most of these devices are very expensive to purchase, making them hampered in wide adoption. Usually, **appearance-based gaze estimation employs a top-down strategy, which predicts the gaze direction or gaze point through eye images directly**. Such an approach is well established as another alternative for eye tracking since only achieving eye images is much cheaper. [1]\n",
    "\n",
    "<img src=\"figures/gaze_est.png\" style=\"zoom:80%\" />\n",
    "\n",
    "Many datasets are built to study this topic, including\n",
    "- [ShanghaiTechGaze+](https://ieeexplore.ieee.org/document/8454246/authors#authors): a multi-view dataset with depth information\n",
    "- [MPIIGaze](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild/): a dataset for estimating gaze direction\n",
    "- [GazeCapture](https://gazecapture.csail.mit.edu/): provides a large-scale(~1474 subjects) on apple devices for estimating end-to-end postion on device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.1.2: Goal\n",
    "\n",
    "In this assignment, you are expected to:\n",
    " - Know how to use `pandas` to learn about statistics of this dataset(30%).\n",
    " - Know how to use `numpy` to implement KNN to do gaze estimation(35%).\n",
    " - Know how to use `numpy` to implement convolution, Sobel(35%) and HOG-vectorization(bonus, 10%).\n",
    " - Have a gaze estimation demo! (In section 2, no points, but with a lot of fun!)\n",
    " \n",
    " <img src=\"figures/demo.png\" style=\"zoom:50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.2.1: Download the MPIIGaze Dataset (csv version)\n",
    "\n",
    "[MPIIGaze dataset(CVPR2015)](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild/) is a dataset for gaze estimation, which contains 213,659 images that we collected from 15 laptop users over several months in their daily life.\n",
    "\n",
    "![](figures/MPIIGaze_Examples.png)\n",
    "\n",
    "For this assignment, we modify the raw dataset into csv format for you to help you study this dataset by pandas. Exectuate following cells to guarantee that you could download the dataset from our mirror(at cells below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: run this cell before runnng any cell to activate auto re-import\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# 'gazelib' is the toolkit provided by this assignment, at the same directory of this notebook\n",
    "from gazelib.utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "download_csv_mpIIdataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_df = load_train_csv_as_df()\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run and read following codes for visualization\n",
    "- Learn how to use `iloc` to get a row by index.\n",
    "- Learn how to use `decode_base64_img` from `gazelib.utils` to decode images from a base64 encoded string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the eye images\n",
    "plt.figure(figsize=(10, 5))\n",
    "# randomly select an index, visualize following 6 images\n",
    "start_idx = max(int(random.random() * len(train_df)) - 6, 0)\n",
    "\n",
    "for offset in range(6):\n",
    "    idx = offset + start_idx\n",
    "    # Here, use iloc to get the row we want\n",
    "    row = train_df.iloc[idx]\n",
    "    \n",
    "    plt.subplot(230 + offset + 1)\n",
    "    # Please remember using decode_base64_img to decode image\n",
    "    img_np = decode_base64_img(row['image_base64'])\n",
    "    subject_id = row['subject_id']\n",
    "    yaw, pitch = row['yaw'], row['pitch']\n",
    "    xyz_gt = yaw_pitch2vec(np.array([yaw, pitch]))\n",
    "    \n",
    "    x_center, y_center = 15, 9\n",
    "    length = 20\n",
    "    \n",
    "    plt.imshow(img_np, cmap=\"gray\", vmin=0, vmax=256)\n",
    "    plt.plot(\n",
    "        [x_center, x_center + xyz_gt[0] * length],\n",
    "        [y_center, y_center + xyz_gt[1] * length],\n",
    "        linewidth=5, c=\"r\", label=\"ground truth\", alpha=0.8\n",
    "    )\n",
    "    plt.scatter([x_center], [y_center], s=40, c='orange')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Subject id: {}, Index: {}\".format(subject_id, idx))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.2.2 Dataset Introduction\n",
    "- `subject_id`: the identifier for each subject\n",
    "- `yaw`: a float in radian measure, indicating the gaze direction(see below figure)\n",
    "- `pitch`: a float in radian measure, indicating the gaze direction(see below figure)\n",
    "- `image_base64`: a string that encodes the image\n",
    "\n",
    "<img src=\"figures/mpii_vis.png\" style=\"zoom:70%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/front_banner_T1S1_new.png)\n",
    "\n",
    "## Section 1: Pandas\n",
    "### Section 1.1: Compute mean yaw & pitch of that guy! (5%)\n",
    "**Goal**: Compute mean of yaw & pitch w.r.t. some subject.\n",
    "\n",
    "**Note**: Please do not try to cheat local judge by modifying its codes or directly assigning the answer.\n",
    "If you do so, you still cannot pass the online judge.\n",
    "\n",
    "<u>Hints</u>:\n",
    "\n",
    "- `pandas.DataFrame.groupby`: [Document](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html?highlight=groupby#pandas.DataFrame.groupby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gazelib.task_1_judge import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete code at `AppearanceGazeEst.py/mean_of_tgt_subject`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Section 1.1 (5%)\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert_eq\n",
    "from AppearanceGazeEst import mean_of_tgt_subject\n",
    "\n",
    "yaw_1, pitch_1 = mean_of_tgt_subject(train_df, 1)\n",
    "yaw_10, pitch_10 = mean_of_tgt_subject(train_df, 10)\n",
    "\n",
    "assert_eq(yaw_1, -0.000763)\n",
    "assert_eq(pitch_1, -0.165573)\n",
    "assert_eq(yaw_10, -0.000714)\n",
    "assert_eq(pitch_10, -0.156507)\n",
    "\n",
    "print(\"You pass the local test - Section 1.1 (5%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: Filter by yaw (5%)\n",
    "\n",
    "**Goal**: Count the number of images that meet the requirement(in comment) of the function `count_tgt_subject`.\n",
    "\n",
    "i.e. A function that counts the number of images of which yaw is larger(>) than yaw_threshold. \n",
    "\n",
    "<u>Hints</u>:\n",
    "\n",
    "- `pandas.DataFrame.loc`: [Document](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete code at `AppearanceGazeEst.py/count_tgt_subject`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Section 1.2 (5%)\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert_eq\n",
    "from AppearanceGazeEst import count_tgt_subject\n",
    "\n",
    "assert_eq(count_tgt_subject(train_df, 0.02), 19571)\n",
    "assert_eq(count_tgt_subject(train_df, -0.02), 22876)\n",
    "\n",
    "print(\"You pass the local test - Section 1.2 (5%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3: Get minimial value of target column (5%)\n",
    "\n",
    "**Goal**: Get minimial value of target column specified by `col` with the function `get_min_val_of_tgt_col`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete code at `AppearanceGazeEst.py/get_min_val_of_tgt_col`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AppearanceGazeEst import get_min_val_of_tgt_col\n",
    "# Local Test - Section 1.3 (5%)\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert_eq\n",
    "assert_eq(get_min_val_of_tgt_col(train_df, 'yaw'), -0.7248519)\n",
    "\n",
    "print(\"You pass the local test - Section 1.3 (5%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.4: Whose eyeball is the most active one? (5%)\n",
    "\n",
    "**Goal**: Implement a function that sorts subject ids by standard deviation of their yaws, pitchs.\n",
    "\n",
    "<u>Hints</u>:\n",
    "\n",
    "- `pandas.DataFrame.sort_values`: [Document](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete code at `AppearanceGazeEst.py/sort_ids_by_stdofcol`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AppearanceGazeEst import sort_ids_by_stdofcol\n",
    "# Local Test - Section 1.4 (5%)\n",
    "# Note: feel free to print out your result to debug if it cannot pass assert\n",
    "assert sort_ids_by_stdofcol(train_df, 'yaw') == [11, 12, 1, 10, 2, 9, 4, 13, 3, 8, 7, 5, 6, 14]\n",
    "\n",
    "print(\"You pass the local test - Section 1.4 (5%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete code at `AppearanceGazeEst.py/compute_mean_eye`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.5: Mean eye is the perfect eye? (5%)\n",
    "\n",
    "**Goal**: Psychological research once indicated that mean face is impressive to most people. At the same time mean face is also involved in [whitening](https://www.datasciencecentral.com/profiles/blogs/preprocessing-for-deep-learning-from-covariance-matrix-to-image) process, which is a popular preprocessing techique in face detection. Now, it is high time that you should compute the mean eye.\n",
    "\n",
    "<u>Hints</u>:\n",
    "\n",
    "- `pandas.DataFrame.iterrows`: [Document](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html)\n",
    "- `decode_base64_img` from gazelib.utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Section 1.5 (5%)\n",
    "from AppearanceGazeEst import compute_mean_eye\n",
    "\n",
    "mean_eye = compute_mean_eye(train_df)\n",
    "check_im_similarity(mean_eye)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.6: Does he or she wear eyeglasses? (5%)\n",
    "\n",
    "**Goal**: Statistics in recent research paper shows that current algorithms are not robust to the subjects with eyeglasses. Imagine you are now a researcher who wants to study this problem. Before diving deeper, you should add a column in current data frame to inform you whether each row's subject wears eyeglasses or not.\n",
    "\n",
    "<img src=\"figures/woglasses.png\" style=\"zoom:50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/code_time.png)\n",
    "**Complete code at `AppearanceGazeEst.py/add_glasses_info`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Test - Section 1.6 (5%)\n",
    "from AppearanceGazeEst import add_glasses_info\n",
    "\n",
    "ret_df = add_glasses_info(train_df)\n",
    "\n",
    "assert not ret_df.loc[0]['has_glasses']\n",
    "assert ret_df.loc[12]['has_glasses']\n",
    "\n",
    "print(\"You pass the local test - Section 1.6 (5%)\")\n",
    "\n",
    "ret_df.head(13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figures/good_job_banner.png)\n",
    "You should have completed **all cells(30%)** in this task locally when you reach here! You have been equipped with skills to get/set anything you want by pandas.\n",
    "\n",
    "**checklist**\n",
    "- mean_of_tgt_subject (5%)\n",
    "- count_tgt_subject (5%)\n",
    "- get_min_val_of_tgt_col (5%)\n",
    "- sort_ids_by_stdofcol (5%)\n",
    "- compute_mean_eye (5%)\n",
    "- add_glasses_info (5%)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "```Bibtex\n",
    "[1] @ARTICLE{\n",
    "    8454246, \n",
    "    author={D. {Lian} and L. {Hu} and W. {Luo} and Y. {Xu} and L. {Duan} and J. {Yu} and S. {Gao}}, \n",
    "    journal={IEEE Transactions on Neural Networks and Learning Systems}, \n",
    "    title={Multiview Multitask Gaze Estimation With Deep Convolutional Neural Networks}, \n",
    "    year={2019}, \n",
    "    volume={30}, \n",
    "    number={10}, \n",
    "    pages={3010-3023}, \n",
    "    keywords={computer vision;convolutional neural nets;estimation theory;gaze tracking;multiview cameras;multiview gaze tracking data;convolutional neural networks architecture;multiview multitask gaze point estimation solution;multiview eye images;gaze direction estimation;deep convolutional neural networks;Estimation;Gaze tracking;Head;Task analysis;Feature extraction;Cameras;Robustness;Convolutional neural networks (CNNs);gaze tracking;multitask learning (MTL);multiview learning}, \n",
    "    doi={10.1109/TNNLS.2018.2865525}, \n",
    "    ISSN={2162-2388}, \n",
    "    month={Oct},\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
